{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImage(path):\n",
    "    # Read image\n",
    "    image_list = []\n",
    "    i = 0\n",
    "    df = pd.read_csv('data/training-a.csv')\n",
    "    filename = df['filename']\n",
    "    labels = df['digit']\n",
    "    # convert to numpy array\n",
    "    labels = np.array(labels)\n",
    "    print(labels)\n",
    "    for filename in os.listdir(path):\n",
    "        img = cv2.imread(os.path.join(path,filename))\n",
    "        # print(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (128, 128))\n",
    "        img = (255-img.transpose(2, 0, 1))/255\n",
    "        image_list.append(img)\n",
    "        i += 1\n",
    "        if i == 1:\n",
    "            break\n",
    "    # print(len(image_list))\n",
    "    return np.array(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getImage('data/training-a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabel(path):\n",
    "    # Read label\n",
    "    df = pd.read_csv(path)\n",
    "    labels = df['digit']\n",
    "    # convert to numpy array\n",
    "    labels = np.array(labels)\n",
    "    print(labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionLayer:\n",
    "    #write code for Convolution\n",
    "    def __init__(self, num_filters, filter_size, stride, padding):\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.filters = None\n",
    "        self.biases = None\n",
    "        self.input_pad = None\n",
    "        self.input = None\n",
    "        self.cache = None\n",
    "        \n",
    "    def getWindows(self, input, output_size, kernel_size, padding=0, stride=1, dilate=0):\n",
    "        working_input = input\n",
    "        working_pad = padding\n",
    "        # dilate the input if necessary\n",
    "        if dilate != 0:\n",
    "            working_input = np.insert(working_input, range(1, input.shape[2]), 0, axis=2)\n",
    "            working_input = np.insert(working_input, range(1, input.shape[3]), 0, axis=3)\n",
    "\n",
    "        # pad the input if necessary\n",
    "        if working_pad != 0:\n",
    "            working_input = np.pad(working_input, pad_width=((0,), (0,), (working_pad,), (working_pad,)), mode='constant', constant_values=(0.,))\n",
    "\n",
    "        in_b, in_c, out_h, out_w = output_size\n",
    "        out_b, out_c, _, _ = input.shape\n",
    "        batch_str, channel_str, kern_h_str, kern_w_str = working_input.strides\n",
    "\n",
    "        return np.lib.stride_tricks.as_strided(\n",
    "            working_input,\n",
    "            (out_b, out_c, out_h, out_w, kernel_size, kernel_size),\n",
    "            (batch_str, channel_str, stride * kern_h_str, stride * kern_w_str, kern_h_str, kern_w_str)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "\n",
    "        # n, c, h, w = input.shape\n",
    "        batch_size, num_channels, height, width = input.shape\n",
    "        output_height = (height - self.filter_size + 2 * self.padding) // self.stride + 1\n",
    "        output_weight = (width - self.filter_size + 2 * self.padding) // self.stride + 1\n",
    "        \n",
    "        if self.filters is None:\n",
    "            self.filters = np.random.randn(self.num_filters, num_channels, self.filter_size, self.filter_size) / np.sqrt(2 / ( num_channels * self.filter_size * self.filter_size))\n",
    "        if self.biases is None:\n",
    "            self.biases = np.random.randn(self.num_filters)\n",
    "\n",
    "        windows = self.getWindows(input, (batch_size, num_channels, output_height, output_weight), self.filter_size, self.padding, self.stride)\n",
    "\n",
    "        output = np.einsum('bihwkl,oikl->bohw', windows, self.filters)\n",
    "\n",
    "        # add bias to kernels\n",
    "        output += self.biases[None, :, None, None]\n",
    "\n",
    "        self.cache = input, windows\n",
    "        print(\"conv forward done. output shape: \", output.shape)\n",
    "        return output\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        input, windows = self.cache\n",
    "\n",
    "        padding = self.filter_size - 1 if self.padding == 0 else self.padding\n",
    "\n",
    "        dout_windows = self.getWindows(output_gradient, input.shape, self.filter_size, padding=padding, stride=1, dilate=self.stride - 1)\n",
    "        rotated_filter = np.rot90(self.filters, 2, axes=(2, 3))\n",
    "\n",
    "        bias_gradient = np.sum(output_gradient, axis=(0, 2, 3))\n",
    "        filter_gradient = np.einsum('bihwkl,bohw->oikl', windows, output_gradient)\n",
    "        input_gradient = np.einsum('bohwkl,oikl->bihw', dout_windows, rotated_filter)\n",
    "\n",
    "        print(\"conv back korlam. \", input_gradient.shape)\n",
    "        return input_gradient\n",
    "    \n",
    "    def update_parameter(self, filter_gradient, bias_gradient, learning_rate):\n",
    "        self.filters -= learning_rate * filter_gradient\n",
    "        self.biases -= learning_rate * bias_gradient\n",
    "        \n",
    "    def _forward(self, input):\n",
    "        self.input = input\n",
    "        batch_size, num_channels, input_height, input_width = input.shape\n",
    "        print(\"batch size: \", batch_size, \"num_channels: \", num_channels, \"input_height: \", input_height, \"input_width: \", input_width)\n",
    "        \n",
    "        if self.filters is None:\n",
    "            self.filters = np.random.randn(self.num_filters, num_channels, self.filter_size, self.filter_size) / np.sqrt(2 / ( num_channels * self.filter_size * self.filter_size))\n",
    "        if self.biases is None:\n",
    "            self.biases = np.random.randn(self.num_filters)   \n",
    "        \n",
    "        print(\"conv e filter size: \", self.filters.shape)    \n",
    "        self.input_pad = np.pad(input, ((0,), (0,), (self.padding,), (self.padding,)), mode='constant')\n",
    "        self.output = np.zeros((batch_size, self.num_filters, int((input_height - self.filter_size + 2*self.padding)/self.stride + 1), int((input_width - self.filter_size + 2*self.padding)/self.stride + 1)))\n",
    "        # print(\"output shape: \", self.output.shape)\n",
    "        for i in range(batch_size):\n",
    "            for j in range(self.num_filters):\n",
    "                for k in range(self.output.shape[2]):\n",
    "                    for l in range(self.output.shape[3]):\n",
    "                        self.output[i, j, k, l] = np.sum(self.filters[j, :, :, :] * self.input_pad[i, :, k*self.stride:k*self.stride+self.filter_size, l*self.stride:l*self.stride+self.filter_size]) + self.biases[j]\n",
    "                        \n",
    "        print(\"output shape: \", self.output.shape)\n",
    "        return self.output\n",
    "    \n",
    "    \n",
    "    def _backward(self, output_gradient, learning_rate):\n",
    "        #write code for backward pass\n",
    "        \n",
    "        batch_size, num_channels, output_height, output_width = output_gradient.shape\n",
    "        output_height_pad = (output_height - 1) * self.stride + 1\n",
    "        output_width_pad = (output_width - 1) * self.stride + 1\n",
    "        \n",
    "        input_height = self.input_pad.shape[2] - 2 * self.padding\n",
    "        input_width = self.input_pad.shape[3] - 2 * self.padding\n",
    "        \n",
    "        print(\"input pad shape: \", self.input_pad.shape)\n",
    "        print(\"input height: \", input_height, \"input width: \", input_width)\n",
    "        \n",
    "        bias_gradient = np.sum(output_gradient, axis=(0, 2, 3)) / batch_size\n",
    "        print(\"output gradient shape: \", output_gradient.shape)\n",
    "        output_gradient_sparse = np.zeros((batch_size, self.num_filters, output_height_pad, output_width_pad))\n",
    "        print(\"output gradient sparse shape: \", output_gradient_sparse.shape)\n",
    "        output_gradient_sparse[:, :, :: self.stride, :: self.stride] = output_gradient\n",
    "        \n",
    "        print(\"input pad shape: \", self.input_pad.shape)\n",
    "        \n",
    "        filters_ = np.rot90(self.filters, 2, (2, 3))  ### confusion!!\n",
    "        filter_gradient = np.zeros((self.num_filters, self.input.shape[1], self.filter_size, self.filter_size))\n",
    "        \n",
    "        for i in range(self.num_filters):\n",
    "                for k in range(self.filter_size):\n",
    "                    for l in range(self.filter_size):\n",
    "                        filter_gradient[i, :, k, l] = np.sum(self.input_pad[:, i:, k:k+output_height_pad, l:l+output_width_pad] * output_gradient_sparse[:, i, :, :], axis=(0, 2, 3))\n",
    "        print(\"conv backprop. filter gradient shape: \", filter_gradient.shape)\n",
    "        \n",
    "        input_gradient = np.zeros((batch_size, self.input.shape[1], input_height, input_width))\n",
    "        print(\"input gradient shape: \", input_gradient.shape)\n",
    "        output_gradient_sparse_pad = np.pad(output_gradient_sparse, ((0,), (0,), (self.filter_size-1 - self.padding,), (self.filter_size-1 - self.padding,)), mode='constant')\n",
    "        print(\"output gradient sparse pad shape: \", output_gradient_sparse_pad.shape)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for j in range(self.input.shape[1]):\n",
    "                for k in range(input_height):\n",
    "                    for l in range(input_width):\n",
    "                        # calculate input gradient using stride\n",
    "                        input_gradient[i, j, k, l] = np.sum(filters_[:, j, :, :] * output_gradient_sparse_pad[i, :, k:k+self.filter_size, l:l+self.filter_size])\n",
    "                        \n",
    "        print(\"conv backprop. input gradient shape: \", input_gradient.shape)\n",
    "        return input_gradient\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLuActivation:\n",
    "    # write code for ReLu activation function\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        print(\"relu korlam\")\n",
    "        return np.maximum(0, input)\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        \n",
    "        input_gradient = output_gradient * (self.input > 0)\n",
    "        print(\"relu back korlam. shape: \", input_gradient.shape)\n",
    "        return input_gradient\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPooling:\n",
    "    #write code for MaxPooling\n",
    "    def __init__(self, pool_size, stride):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        batch_size, num_channels, input_height, input_width = input.shape\n",
    "        print(\"batch size: \", batch_size, \"num_channels: \", num_channels, \"input_height: \", input_height, \"input_width: \", input_width)\n",
    "        \n",
    "        # print(\"before max pooling: \", input)\n",
    "        self.output = np.zeros((batch_size, num_channels, int((input_height - self.pool_size)/self.stride + 1), int((input_width - self.pool_size)/self.stride + 1)))\n",
    "        for i in range(batch_size):\n",
    "            for j in range(num_channels):\n",
    "                for k in range(self.output.shape[2]):\n",
    "                    for l in range(self.output.shape[3]):\n",
    "                        self.output[i, j, k, l] = np.max(input[i, j, k*self.stride:k*self.stride+self.pool_size, l*self.stride:l*self.stride+self.pool_size])\n",
    "        print(\"max pooling done. shape: \", self.output.shape)\n",
    "        # print(\"after max pooling: \", self.output)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        input_gradient = np.zeros(self.input.shape)\n",
    "        batch_size, num_channels, input_height, input_width = output_gradient.shape\n",
    "        for i in range(batch_size):\n",
    "            for j in range(num_channels):\n",
    "                for k in range(input_height):\n",
    "                    for l in range(input_width):\n",
    "                        max_index = np.argmax(self.input[i, j, k*self.stride:k*self.stride+self.pool_size, l*self.stride:l*self.stride+self.pool_size])\n",
    "                        max_index = np.unravel_index(max_index, (self.pool_size, self.pool_size))\n",
    "                        input_gradient[i, j, k*self.stride:k*self.stride+self.pool_size, l*self.stride:l*self.stride+self.pool_size][max_index] = output_gradient[i, j, k, l]\n",
    "            \n",
    "        print(\"max pooling back done. shape: \", input_gradient.shape)\n",
    "        # print(\"after max pooling backprop: \", input_gradient)\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flattening:\n",
    "    #write code for Flattening\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        \n",
    "    def forward(self, input):\n",
    "        print(\"dhuksi\")\n",
    "        self.input = input\n",
    "        self.output = input.reshape(input.shape[0], -1)\n",
    "        print(\"flattening e dhuksi. shape: \", self.output.shape)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        input_gradient = output_gradient.reshape(self.input.shape)\n",
    "        print(\"flattening er backprop e dhukse. shape: \", input_gradient.shape)\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer:\n",
    "    #write code for FullyConnectedLayer\n",
    "    def __init__(self, num_units):\n",
    "        self.num_units = num_units\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.input = None\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        if self.weights is None:\n",
    "            self.weights = np.random.randn(input.shape[1], self.num_units)\n",
    "        if self.bias is None:\n",
    "            self.bias = np.zeros(self.num_units)\n",
    "            \n",
    "        print(\"input shape: \", input.shape)\n",
    "        print(\"weights shape: \", self.weights.shape)\n",
    "        print(\"bias shape: \", self.bias.shape)\n",
    "    \n",
    "        self.output = np.dot(input, self.weights) + self.bias\n",
    "        print(\"fully connected e dhuksi. shape: \", self.output.shape)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        self.weights_gradient = np.dot(self.input.T, output_gradient)/output_gradient.shape[1]\n",
    "        self.bias_gradient = np.mean(output_gradient, axis=0)\n",
    "        input_gradient = np.dot(output_gradient, self.weights.T)\n",
    "        self.update(learning_rate)\n",
    "        print(\"fc backprop done. input gradient shape: \", input_gradient.shape)\n",
    "        return input_gradient\n",
    "    \n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        self.weights -= learning_rate * self.weights_gradient\n",
    "        print(\"fc te weight shape.\", self.weights.shape)\n",
    "        self.bias -= learning_rate * self.bias_gradient\n",
    "        self.weights_gradient = np.zeros(self.weights.shape)\n",
    "        self.bias_gradient = np.zeros(self.bias.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax:\n",
    "    #write code for SoftMax\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __str__(self):\n",
    "        return \"Softmax\"\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        #normalize input\n",
    "        self.input -= np.max(self.input, axis=1, keepdims=True)\n",
    "        self.output = np.exp(self.input)\n",
    "        self.output /= np.sum(self.output, axis=1, keepdims=True)\n",
    "        print(\"forward sheshhhhh!!!!!!\")\n",
    "        return self.output\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        print(\"softmax er backprop dhukse. shape: \", output_gradient.shape)\n",
    "        return output_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    layers = []\n",
    "    layers.append(ConvolutionLayer(3, 3, 1, 1))\n",
    "    layers.append(ReLuActivation())\n",
    "    layers.append(MaxPooling(2, 2))\n",
    "    layers.append(ConvolutionLayer(3, 3, 1, 1))\n",
    "    layers.append(ReLuActivation())\n",
    "    layers.append(MaxPooling(2, 2))\n",
    "    layers.append(Flattening())\n",
    "    layers.append(FullyConnectedLayer(10))\n",
    "    layers.append(SoftMax())\n",
    "    print(\"model created\")\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(layers, input):\n",
    "    for layer in layers:\n",
    "        input = layer.forward(input)\n",
    "    return input\n",
    "\n",
    "def backward_propagation(layers, output_gradient, learning_rate):\n",
    "    for layer in reversed(layers):\n",
    "        output_gradient = layer.backward(output_gradient, learning_rate)\n",
    "        \n",
    "def update_params(layers, learning_rate):\n",
    "    for layer in layers:\n",
    "        layer.update(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model created\n",
      "conv forward done. output shape:  (2, 3, 20, 20)\n",
      "relu korlam\n",
      "batch size:  2 num_channels:  3 input_height:  20 input_width:  20\n",
      "max pooling done. shape:  (2, 3, 10, 10)\n",
      "conv forward done. output shape:  (2, 3, 10, 10)\n",
      "relu korlam\n",
      "batch size:  2 num_channels:  3 input_height:  10 input_width:  10\n",
      "max pooling done. shape:  (2, 3, 5, 5)\n",
      "dhuksi\n",
      "flattening e dhuksi. shape:  (2, 75)\n",
      "input shape:  (2, 75)\n",
      "weights shape:  (75, 10)\n",
      "bias shape:  (10,)\n",
      "fully connected e dhuksi. shape:  (2, 10)\n",
      "forward sheshhhhh!!!!!!\n",
      "softmax er backprop dhukse. shape:  (2, 10)\n",
      "fc te weight shape. (75, 10)\n",
      "fc backprop done. input gradient shape:  (2, 75)\n",
      "flattening er backprop e dhukse. shape:  (2, 3, 5, 5)\n",
      "max pooling back done. shape:  (2, 3, 10, 10)\n",
      "relu back korlam. shape:  (2, 3, 10, 10)\n",
      "max pooling back done. shape:  (2, 3, 20, 20)\n",
      "relu back korlam. shape:  (2, 3, 20, 20)\n"
     ]
    }
   ],
   "source": [
    "input_shape = (2, 10, 20, 20)\n",
    "input = np.random.randint(-10,10,size = input_shape)\n",
    "# print(\"input matrix: \", input)\n",
    "y = [1]\n",
    "layers = model()\n",
    "conv_forward_output = forward_propagation(layers, input)\n",
    "gradient = conv_forward_output - y\n",
    "conv_back_output = backward_propagation(layers, gradient, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(layers, test_input, test_output):\n",
    "    test_output_pred = forward_propagation(layers, test_input)\n",
    "    test_output_pred = np.argmax(test_output_pred, axis=1)\n",
    "    test_output = np.argmax(test_output, axis=1)\n",
    "    accuracy = np.sum(test_output_pred == test_output) / test_output.shape[0]\n",
    "    print(\"accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(layers, X_train, Y_train, X_test, Y_test, num_epochs, learning_rate):\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"epoch: \", epoch)\n",
    "        output = forward_propagation(layers, X_train)\n",
    "        loss = np.sum(-Y_train * np.log(output))\n",
    "        print(\"loss: \", loss)\n",
    "        output_gradient = output - Y_train\n",
    "        backward_propagation(layers, output_gradient)\n",
    "        update_params(layers, learning_rate)\n",
    "        test_model(layers, X_test, Y_test)\n",
    "    print(\"training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the train model\n",
    "layers = model()\n",
    "X_train = getImage('data/training-a/')\n",
    "Y_train = getLabel('data/training-a.csv')\n",
    "X_test = getImage('data/training-b/')\n",
    "Y_test = getLabel('data/training-b.csv')\n",
    "train_model(layers, X_train, Y_train, X_test, Y_test, 1, 0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
