# -*- coding: utf-8 -*-
"""1705091_train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17neiYCrYIhsR__TzNqFbwuWosDZGASzk
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cv2
import os
from sklearn.metrics import log_loss
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix
from tqdm import tqdm 
import pickle

def One_Hot_Encoding(labels):
    encoding_list = []
    for i in range(10):
        encoding = np.zeros(10, dtype=int)
        encoding[i] = 1
        encoding_list.append(encoding)
    
    encoded_label_list = []
    for label in labels:
        encoded_label = encoding_list[label]
        encoded_label_list.append(encoded_label)
        
    return np.array(encoded_label_list)


def getImage(path, filename):
    # Read image
    image_list = []
    
    for file in filename:
        img = cv2.imread(os.path.join(path,file))
        # print(img)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, (28, 28))
        img = (255-img.transpose(2, 0, 1))/255
        image_list.append(img)
       
    # print(len(image_list))
    return np.array(image_list)

class ConvolutionLayer:
    #write code for Convolution
    def __init__(self, num_filters, filter_size, stride, padding):
        self.num_filters = num_filters
        self.filter_size = filter_size
        self.stride = stride
        self.padding = padding
        self.filters = None
        self.biases = None
        self.cache = None
        
    def getWindows(self, input, output_size, kernel_size, padding=0, stride=1, dilate=0):
        working_input = input
        working_pad = padding
        # dilate the input if necessary
        if dilate != 0:
            working_input = np.insert(working_input, range(1, input.shape[2]), 0, axis=2)
            working_input = np.insert(working_input, range(1, input.shape[3]), 0, axis=3)

        # pad the input if necessary
        if working_pad != 0:
            working_input = np.pad(working_input, pad_width=((0,), (0,), (working_pad,), (working_pad,)), mode='constant', constant_values=(0.,))

        in_b, in_c, out_h, out_w = output_size
        out_b, out_c, _, _ = input.shape
        batch_str, channel_str, kern_h_str, kern_w_str = working_input.strides

        return np.lib.stride_tricks.as_strided(
            working_input,
            (out_b, out_c, out_h, out_w, kernel_size, kernel_size),
            (batch_str, channel_str, stride * kern_h_str, stride * kern_w_str, kern_h_str, kern_w_str)
        )
        
        
    def forward(self, input):

        batch_size, num_channels, height, width = input.shape
        output_height = (height - self.filter_size + 2 * self.padding) // self.stride + 1
        output_weight = (width - self.filter_size + 2 * self.padding) // self.stride + 1
        
        if self.filters is None:
            self.filters = np.random.randn(self.num_filters, num_channels, self.filter_size, self.filter_size) / np.sqrt(2 / ( num_channels * self.filter_size * self.filter_size))
        if self.biases is None:
            self.biases = np.random.randn(self.num_filters)

        windows = self.getWindows(input, (batch_size, num_channels, output_height, output_weight), self.filter_size, self.padding, self.stride)

        output = np.einsum('bihwkl,oikl->bohw', windows, self.filters)

        # add bias to kernels
        output += self.biases[None, :, None, None]
        

        self.cache = input, windows
        # print("conv forward done. output shape: ", output.shape)
        return output
   

    def backward(self, output_gradient, learning_rate):
        input, windows = self.cache

        padding = self.filter_size - 1 if self.padding == 0 else self.padding

        dout_windows = self.getWindows(output_gradient, input.shape, self.filter_size, padding=padding, stride=1, dilate=self.stride - 1)
        rotated_filter = np.rot90(self.filters, 2, axes=(2, 3))

        bias_gradient = np.sum(output_gradient, axis=(0, 2, 3))
        filter_gradient = np.einsum('bihwkl,bohw->oikl', windows, output_gradient)
        input_gradient = np.einsum('bohwkl,oikl->bihw', dout_windows, rotated_filter)

        # print("conv back korlam. ", input_gradient.shape)
        self.update_parameter(filter_gradient, bias_gradient, learning_rate)
        return filter_gradient
    
    def update_parameter(self, filter_gradient, bias_gradient, learning_rate):
        self.filters -= learning_rate * filter_gradient
        self.biases -= learning_rate * bias_gradient
        # print("conv e parameter update korlam")

class ReLuActivation:
    # write code for ReLu activation function
    def __init__(self):
        pass
    def forward(self, input):
        self.input = input
        # print("relu korlam")
        return np.maximum(0, input)
    def backward(self, output_gradient, learning_rate):
        
        input_gradient = output_gradient * (self.input > 0)
        # print("relu back korlam. shape: ", input_gradient.shape)
        return input_gradient

class MaxPooling:
    #write code for MaxPooling
    def __init__(self, pool_size, stride):
        self.pool_size = pool_size
        self.stride = stride
        self.input = None
        
    def forward(self, input):
        self.input = input
        batch_size, num_channels, input_height, input_width = input.shape
        # print("batch size: ", batch_size, "num_channels: ", num_channels, "input_height: ", input_height, "input_width: ", input_width)
        
        # print("before max pooling: ", input)
        self.output = np.zeros((batch_size, num_channels, int((input_height - self.pool_size)/self.stride + 1), int((input_width - self.pool_size)/self.stride + 1)))
        for i in range(batch_size):
            for j in range(num_channels):
                for k in range(self.output.shape[2]):
                    for l in range(self.output.shape[3]):
                        self.output[i, j, k, l] = np.max(input[i, j, k*self.stride:k*self.stride+self.pool_size, l*self.stride:l*self.stride+self.pool_size])
        # print("max pooling done. shape: ", self.output.shape)
        # print("after max pooling: ", self.output)
        return self.output
    
    def backward(self, output_gradient, learning_rate):
        input_gradient = np.zeros(self.input.shape)
        batch_size, num_channels, input_height, input_width = output_gradient.shape
        for i in range(batch_size):
            for j in range(num_channels):
                for k in range(input_height):
                    for l in range(input_width):
                        max_index = np.argmax(self.input[i, j, k*self.stride:k*self.stride+self.pool_size, l*self.stride:l*self.stride+self.pool_size])
                        max_index = np.unravel_index(max_index, (self.pool_size, self.pool_size))
                        input_gradient[i, j, k*self.stride:k*self.stride+self.pool_size, l*self.stride:l*self.stride+self.pool_size][max_index] = output_gradient[i, j, k, l]
            
        # print("max pooling back done. shape: ", input_gradient.shape)
        # print("after max pooling backprop: ", input_gradient)
        return input_gradient

class Flattening:
    #write code for Flattening
    def __init__(self):
        self.input = None
        
    def forward(self, input):
        # print("dhuksi")
        self.input = input
        self.output = input.reshape(input.shape[0], -1)
        # print("flattening e dhuksi. shape: ", self.output.shape)
        return self.output
    
    def backward(self, output_gradient, learning_rate):
        input_gradient = output_gradient.reshape(self.input.shape)
        # print("flattening er backprop e dhukse. shape: ", input_gradient.shape)
        return input_gradient

class FullyConnectedLayer:
    #write code for FullyConnectedLayer
    def __init__(self, num_units):
        self.num_units = num_units
        self.weights = None
        self.bias = None
        self.input = None
        
    def forward(self, input):
        self.input = input
        if self.weights is None:
            self.weights = np.random.randn(input.shape[1], self.num_units)
        
        if self.bias is None:
            self.bias = np.zeros(self.num_units)
        
        # print("input shape: ", input.shape)
        # print("weights shape: ", self.weights.shape)
        # print("bias shape: ", self.bias.shape)
    
        self.output = np.dot(input, self.weights) + self.bias
        # print("fully connected e dhuksi. ", self.output)
        return self.output
    
    def backward(self, output_gradient, learning_rate):
        self.weights_gradient = np.dot(self.input.T, output_gradient)/output_gradient.shape[1]
        self.bias_gradient = np.mean(output_gradient, axis=0)
        input_gradient = np.dot(output_gradient, self.weights.T)
        self.update_parameter(learning_rate)
        # print("fc backprop done. input gradient shape: ", input_gradient.shape)
        return input_gradient
    
    
    def update_parameter(self, learning_rate):
        self.weights -= learning_rate * self.weights_gradient
        # print("fc te weight shape.", self.weights.shape)
        self.bias -= learning_rate * self.bias_gradient
        self.weights_gradient = np.zeros(self.weights.shape)
        self.bias_gradient = np.zeros(self.bias.shape)
        # print("fc te parameter update korlam")

class SoftMax:
    #write code for SoftMax
    def __init__(self):
        pass
    def __str__(self):
        return "Softmax"
    def forward(self, input):
        self.input = input
        #normalize input
        self.input -= np.max(self.input, axis=1, keepdims=True)
        self.output = np.exp(self.input)
        self.output /= np.sum(self.output, axis=1, keepdims=True)
        # print("forward sheshhhhh!!!!!!")
        return self.output
    def backward(self, output_gradient, learning_rate):
        # print("softmax er backprop dhukse. shape: ", output_gradient.shape)
        return output_gradient


class CNN_model:
    
    def __init__(self):
        self.layers = []
        
    def create_layers(self, layer):
        self.layers.append(layer)
        
    def forward_propagation(self, input):
        for layer in self.layers:
            input = layer.forward(input)
        return input

    def backward_propagation(self, output_gradient, learning_rate):
        for layer in reversed(self.layers):
            output_gradient = layer.backward(output_gradient, learning_rate)
        return output_gradient


    def validation_model(self, test_input, test_output):
        # print("validation cholche")
        valid_output_pred = self.forward_propagation(test_input)
        # print("predicted output before argmax", valid_output_pred)
        # print("predicted output shape: ", valid_output_pred.shape)
        valid_output_pred = np.argmax(valid_output_pred, axis=1)
        print("predicted output: ", valid_output_pred)
        valid_output = np.argmax(test_output, axis=1)
        print("actual output: ", valid_output)
        
        
        # print("test output shape: ", test_output.shape)
        
        accuracy = accuracy_score(valid_output, valid_output_pred)
        f1_score_value = f1_score(valid_output, valid_output_pred, average='macro')
        # validation_loss = log_loss(test_output, valid_output_pred)
        print("accuracy: ", accuracy)
        print("f1 score: ", f1_score_value)
        
        return accuracy, f1_score_value

    def train(self, X_train, y_train, learning_rate, batch_size):
        
        forward_output = self.forward_propagation(X_train)
        # forward_output_list.append(forward_output)
        batchwise_loss = log_loss(y_train, forward_output)
        # print("batchwise loss: ", batchwise_loss)

        output_gradient = (forward_output - y_train) / batch_size
        conv_weights_gradient = self.backward_propagation(output_gradient, learning_rate)
        
        return batchwise_loss, conv_weights_gradient
    
    def test(self, filelist, X_test, y_test):
        test_output_pred = self.forward_propagation(X_test)
        test_output_pred = np.argmax(test_output_pred, axis=1)
        test_output = np.argmax(y_test, axis=1)
        accuracy = accuracy_score(test_output, test_output_pred)
        f1_score_value = f1_score(test_output, test_output_pred, average='macro')
        confusion_matrix_value = confusion_matrix(test_output, test_output_pred)
        print("accuracy: ", accuracy)
        print("f1 score: ", f1_score_value)
        print("confusion matrix: ", confusion_matrix_value)
        
        test_result = pd.DataFrame({'filename': filelist, 'digit': test_output_pred})
        test_result.to_csv('1705091_prediction.csv', index=False)
        print("test result saved")
        

if __name__ == '__main__': 
    # call the train model
    # csv_file1 = pd.read_csv('data/training-a.csv')
    # csv_file2 = pd.read_csv('data/training-b.csv')
    csv_file3 = pd.read_csv('data/training-c.csv')

    # csv_file1 = csv_file1.drop(csv_file1.columns[[1,2,4,5,6]], axis=1)
    # csv_file2 = csv_file2.drop(csv_file2.columns[[1,2,4,5,6]], axis=1)
    csv_file3 = csv_file3.drop(csv_file3.columns[[1,2,4,5,6]], axis=1)

    # csv_file = pd.concat([csv_file1, csv_file2, csv_file3])
    # print("csv file: ", csv_file)
    csv_file = csv_file3

    labels = csv_file['digit']
    labels = np.array(labels)
    filename = csv_file['filename']
    encoded_label_list = One_Hot_Encoding(labels)


    model = CNN_model()
    model.create_layers(ConvolutionLayer(3, 3, 2, 1))
    model.create_layers(ReLuActivation())
    model.create_layers(MaxPooling(2, 2))
    model.create_layers(ConvolutionLayer(3, 3, 2, 1))
    model.create_layers(ReLuActivation())
    model.create_layers(MaxPooling(2, 2))
    model.create_layers(Flattening())
    model.create_layers(FullyConnectedLayer(120))
    model.create_layers(ReLuActivation())
    model.create_layers(FullyConnectedLayer(84))
    model.create_layers(ReLuActivation())
    model.create_layers(FullyConnectedLayer(10))
    model.create_layers(ReLuActivation())
    model.create_layers(SoftMax())
    print("model created") 
    
    learning_rate = 0.0000001
    batch_size = 8
    num_epochs = 20

    train_loss_list = []
    valid_loss_list = []
    accuracy_list = []
    f1_score_list = []
    for epoch in range(num_epochs):
        X_train, X_valid, Y_train, Y_valid = train_test_split(filename, encoded_label_list, test_size=0.2, shuffle=True)
    
        X_valid = getImage('data/training-c/', X_valid)
        forward_output_list = []
        output_gradient_list = []
        conv_weights_gradient_list = []
        loss = 0
        
        print("epoch: ", epoch+1)
        for i in tqdm(range(0, len(Y_train), batch_size)):
            batch_X_train = getImage('data/training-c/', X_train[i:i+batch_size])
            batch_Y_train = Y_train[i:i+batch_size]
            batchwise_loss, conv_weights_gradient =  model.train(batch_X_train, batch_Y_train, learning_rate, batch_size)
            loss += batchwise_loss
            conv_weights_gradient_list.append(conv_weights_gradient)
            
        print("loss: ", loss)
        train_loss_list.append(loss)
        # print("conv weights gradient list ", conv_weights_gradient_list)
        
        accuracy, f1_score_value = model.validation_model(X_valid, Y_valid)
        accuracy_list.append(accuracy)
        f1_score_list.append(f1_score_value)
        # valid_loss_list.append(validation_loss)
        
        
    print("training complete")
    print("train loss list: ", train_loss_list)
    print("accuracy list: ", accuracy_list)
    print("f1 score list: ", f1_score_list)

    model.layers[0].cache = None
    model.layers[2].input = None
    model.layers[3].cache = None
    model.layers[7].num_units = None
    model.layers[8].num_units = None
    model.layers[9].num_units = None
    

    pickle.dump(model, open('1705091_model.pickle', 'wb'))
    print("pickle file created")
    
    # plt.figure(figsize=(6, 6))
    # plt.title('Metrics vs. epoch')
    # plt.plot(range(1, num_epochs+1), train_loss_list, 'r')
    # plt.plot(range(1, num_epochs+1), accuracy_list, 'b')
    # plt.plot(range(1, num_epochs+1), f1_score_value, 'g')
    # plt.xlabel('epoch')
    # plt.show()
    